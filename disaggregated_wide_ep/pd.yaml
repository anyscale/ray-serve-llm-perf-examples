# PD + DP configuration (Prefill: DPEP16, Decode: DPEP32) with NIXL KV transfer
# Uses NIXL connector for GPU-to-GPU KV transfer with Data + Expert Parallelism
# Run: `serve run pd.yaml`
applications:
  - name: deepseek-pd-wide-ep
    route_prefix: /
    import_path: pd_dpep_builder:build_pd_dp_openai_app
    args:
      prefill_config:
        model_loading_config:
          model_id: deepseek
          model_source: deepseek-ai/DeepSeek-V3
        deployment_config:
          ray_actor_options:
            resources:
              prefill: 1
          request_router_config:
            request_router_class: ray.serve.llm.request_router:PrefixCacheAffinityRouter
        engine_kwargs:
          max_model_len: 10000
          data_parallel_size: 16
          tensor_parallel_size: 1
          enable_expert_parallel: true
          kv_transfer_config:
            kv_connector: NixlConnector
            kv_role: kv_both
        experimental_configs:
          dp_size_per_node: 8
        runtime_env:
          env_vars:
            VLLM_USE_DEEP_GEMM: "1"
            VLLM_ALL2ALL_BACKEND: "deepep_high_throughput"
      decode_config:
        model_loading_config:
          model_id: deepseek
          model_source: deepseek-ai/DeepSeek-V3
        deployment_config:
          ray_actor_options:
            resources:
              decode: 1
        engine_kwargs:
          max_model_len: 10000
          data_parallel_size: 32
          tensor_parallel_size: 1
          enable_expert_parallel: true
          kv_transfer_config:
            kv_connector: NixlConnector
            kv_role: kv_both
        experimental_configs:
          dp_size_per_node: 8
        runtime_env:
          env_vars:
            VLLM_USE_DEEP_GEMM: "1"
            VLLM_ALL2ALL_BACKEND: "deepep_low_latency"
      ingress_deployment_config:
        autoscaling_config:
          min_replicas: 16
          max_replicas: 16
working_dir: .
query_auth_token_enabled: false
name: deepseek-pd-wide-ep