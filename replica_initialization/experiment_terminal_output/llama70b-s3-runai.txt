INFO 10-15 12:04:02 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:OpenAiIngress pid=136421)[0m INFO 10-15 12:05:14 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:05:19 [__init__.py:224] Automatically detected platform cuda.
[36m(download_model_files pid=136858)[0m INFO 10-15 12:05:26 [__init__.py:224] Automatically detected platform cuda.
[36m(_get_vllm_engine_config pid=136858)[0m INFO 10-15 12:05:30 [model.py:646] Resolved architecture: LlamaForCausalLM
[36m(_get_vllm_engine_config pid=136858)[0m INFO 10-15 12:05:30 [model.py:1734] Using max model len 8192
[36m(_get_vllm_engine_config pid=136858)[0m INFO 10-15 12:05:31 [arg_utils.py:1348] Using ray runtime env (env vars redacted): {'cgroupv2': {}, 'ray_debugger': {'working_dir': '/home/ray/default/workspace'}, 'working_dir': 'gcs://_ray_pkg_059ca8f863d8493995629102b17d6c03e84de0c1.zip', 'env_vars': {'AWS_ACCESS_KEY_ID': '***', 'AWS_SECRET_ACCESS_KEY': '***', 'AWS_SESSION_TOKEN': '***', 'VLLM_USE_V1': '***'}, 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}
[36m(_get_vllm_engine_config pid=136858)[0m INFO 10-15 12:05:31 [scheduler.py:225] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m WARNING 10-15 12:05:31 [__init__.py:2881] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:05:35 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:37 [core.py:734] Waiting for init message from front-end.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:37 [core.py:97] Initializing a V1 LLM engine (v0.1.dev10446+g2dcd12d35) with config: model='/home/ray/.cache/vllm/assets/model_streamer/b6142c80', speculative_config=None, tokenizer='/home/ray/.cache/vllm/assets/model_streamer/b6142c80', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=runai_streamer, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llama, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': 3, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention', 'vllm::sparse_attn_indexer'], 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 512, 'local_cache_dir': None}
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:37 [ray_utils.py:351] Using the existing placement group
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:37 [ray_distributed_executor.py:180] use_ray_spmd_worker: True
[36m(pid=137283)[0m INFO 10-15 12:05:42 [__init__.py:224] Automatically detected platform cuda.
[36m(pid=137285)[0m INFO 10-15 12:05:42 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:45 [ray_env.py:66] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:45 [ray_env.py:69] Copying the following environment variables to workers: ['VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1', 'VLLM_USE_RAY_SPMD_WORKER', 'LD_LIBRARY_PATH', 'VLLM_USE_RAY_COMPILED_DAG']
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:05:45 [ray_env.py:74] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=137282)[0m WARNING 10-15 12:05:52 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(pid=137283)[0m INFO 10-15 12:05:42 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:54 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:56 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:56 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1528e102'), local_subscribe_addr='ipc:///tmp/45e81b1c-dfbc-43fc-aecf-42d55616d7cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:56 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [parallel_state.py:1325] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(RayWorkerWrapper pid=137285)[0m WARNING 10-15 12:05:52 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/b6142c80...
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=137286)[0m [RunAI Streamer] CPU Buffer size: 240 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors']
[36m(RayWorkerWrapper pid=137285)[0m [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[32m [repeated 23x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m [RunAI Streamer] CPU Buffer size: 82.7 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors']
[36m(RayWorkerWrapper pid=137286)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors']
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(pid=137286)[0m INFO 10-15 12:05:43 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:54 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:56 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m WARNING 10-15 12:05:52 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[32m [repeated 23x across cluster][0m
[36m(RayWorkerWrapper pid=137282)[0m Read throughput is 1.81 GB per second 
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:56 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:56 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1528e102'), local_subscribe_addr='ipc:///tmp/45e81b1c-dfbc-43fc-aecf-42d55616d7cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [parallel_state.py:1325] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m WARNING 10-15 12:05:52 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/b6142c80...[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:05:58 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m [RunAI Streamer] CPU Buffer size: 240 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors'][32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m [RunAI Streamer] CPU Buffer size: 82.7 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors'][32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors'][32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m 
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:05:56 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:05:58 [parallel_state.py:1325] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:05:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:05:58 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/b6142c80...[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:05:58 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m [RunAI Streamer] CPU Buffer size: 240 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors'][32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m [RunAI Streamer] CPU Buffer size: 82.7 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors'][32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B/model-00001-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00002-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00003-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00004-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00005-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00006-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00007-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00008-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00009-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00010-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00011-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00012-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00013-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00014-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00015-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00016-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00017-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00018-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00019-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00020-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00021-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00022-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00023-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00024-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00025-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00026-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00027-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00028-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00029-of-00030.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B/model-00030-of-00030.safetensors'][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m Read throughput is 1.67 GB per second [32m [repeated 5x across cluster][0m
[36m(RayWorkerWrapper pid=137285)[0m Read throughput is 1.77 GB per second [32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=137282)[0m Read throughput is 1.48 GB per second [32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=137283)[0m Read throughput is 1.66 GB per second [32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m Read throughput is 1.09 GB per second [32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137283)[0m Read throughput is 1.55 GB per second 
[36m(RayWorkerWrapper pid=137282)[0m Read throughput is 1.54 GB per second 
[36m(RayWorkerWrapper pid=137285)[0m Read throughput is 1.59 GB per second [32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137282)[0m [RunAI Streamer] Overall time to stream 131.4 GiB of all files: 91.06s, 1.4 GiB/s
[36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:31 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 91.883922 seconds
[36m(RayWorkerWrapper pid=137283)[0m INFO 10-15 12:07:43 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/6622fbebc9/rank_2_0/backbone for vLLM's torch.compile
[36m(RayWorkerWrapper pid=137283)[0m INFO 10-15 12:07:43 [backends.py:622] Dynamo bytecode transform time: 12.22 s
[36m(RayWorkerWrapper pid=137283)[0m Read throughput is 1.81 GB per second [32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=137285)[0m [RunAI Streamer] Overall time to stream 131.4 GiB of all files: 91.22s, 1.4 GiB/s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:07:31 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 92.096867 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:07:47 [backends.py:247] Cache the graph for dynamic shape for later use
[36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:08:32 [backends.py:274] Compiling a graph for dynamic shape takes 47.35 s
[36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:44 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/6622fbebc9/rank_1_0/backbone for vLLM's torch.compile[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:44 [backends.py:622] Dynamo bytecode transform time: 12.70 s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:47 [backends.py:247] Cache the graph for dynamic shape for later use[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:08:42 [monitor.py:33] torch.compile takes 61.94 s in total
[36m(RayWorkerWrapper pid=137283)[0m INFO 10-15 12:08:34 [backends.py:274] Compiling a graph for dynamic shape takes 49.95 s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:08:45 [gpu_worker.py:315] Available KV cache memory: 35.76 GiB
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m 
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m Read throughput is 1.67 GB per second [32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m Read throughput is 1.77 GB per second [32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m Read throughput is 1.48 GB per second [32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137283)[0m Read throughput is 1.66 GB per second [32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m Read throughput is 1.09 GB per second [32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137283)[0m Read throughput is 1.55 GB per second 
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m Read throughput is 1.54 GB per second 
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m Read throughput is 1.59 GB per second [32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m [RunAI Streamer] Overall time to stream 131.4 GiB of all files: 91.06s, 1.4 GiB/s
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:31 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 91.883922 seconds
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137283)[0m INFO 10-15 12:07:43 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/6622fbebc9/rank_2_0/backbone for vLLM's torch.compile
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137283)[0m INFO 10-15 12:07:43 [backends.py:622] Dynamo bytecode transform time: 12.22 s
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137283)[0m Read throughput is 1.81 GB per second [32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m [RunAI Streamer] Overall time to stream 131.4 GiB of all files: 91.22s, 1.4 GiB/s[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:07:31 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 92.096867 seconds[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:07:47 [backends.py:247] Cache the graph for dynamic shape for later use
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:44 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/6622fbebc9/rank_1_0/backbone for vLLM's torch.compile[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:44 [backends.py:622] Dynamo bytecode transform time: 12.70 s[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:07:47 [backends.py:247] Cache the graph for dynamic shape for later use[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137283)[0m INFO 10-15 12:08:34 [backends.py:274] Compiling a graph for dynamic shape takes 49.95 s[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:08:45 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:09:01 [custom_all_reduce.py:216] Registering 21252 cuda graph addresses
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:08:42 [monitor.py:33] torch.compile takes 61.94 s in total[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137282)[0m INFO 10-15 12:08:32 [backends.py:274] Compiling a graph for dynamic shape takes 47.35 s
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137286)[0m INFO 10-15 12:08:45 [gpu_worker.py:315] Available KV cache memory: 35.76 GiB[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:09:02 [gpu_model_runner.py:3816] Graph capturing finished in 16 secs, took 1.05 GiB
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:08:42 [monitor.py:33] torch.compile takes 60.89 s in total[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m [36m(RayWorkerWrapper pid=137285)[0m INFO 10-15 12:08:45 [gpu_worker.py:315] Available KV cache memory: 35.76 GiB[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:09:02 [core.py:243] init engine (profile, create kv cache, warmup model) took 91.10 seconds
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:09:03 [loggers.py:191] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 29297
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m [1;36m(EngineCore_DP0 pid=137102)[0;0m INFO 10-15 12:09:03 [gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:09:03 [api_server.py:1629] Supported tasks: ['generate']
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m WARNING 10-15 12:09:03 [model.py:1591] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:09:03 [serving_responses.py:148] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:09:03 [serving_chat.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:09:03 [serving_completion.py:67] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m INFO 10-15 12:09:04 [chat_utils.py:545] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256] Error in preprocessing prompt inputs
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256] Traceback (most recent call last):
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]   File "/home/ray/default/repo/vllm/vllm/entrypoints/openai/serving_chat.py", line 234, in create_chat_completion
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]     ) = await self._preprocess_chat(
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]   File "/home/ray/default/repo/vllm/vllm/entrypoints/openai/serving_engine.py", line 1085, in _preprocess_chat
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]     request_prompt = apply_hf_chat_template(
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]                      ^^^^^^^^^^^^^^^^^^^^^^^
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]   File "/home/ray/default/repo/vllm/vllm/entrypoints/chat_utils.py", line 1535, in apply_hf_chat_template
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256]     raise ValueError(
[36m(ServeReplica:default:LLMServer:llama pid=136671)[0m ERROR 10-15 12:09:04 [serving_chat.py:256] ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.
