INFO 10-15 15:40:51 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:OpenAiIngress pid=351361)[0m INFO 10-15 15:42:03 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:42:08 [__init__.py:224] Automatically detected platform cuda.
[36m(download_model_files pid=351782)[0m INFO 10-15 15:42:15 [__init__.py:224] Automatically detected platform cuda.
[36m(_get_vllm_engine_config pid=351782)[0m INFO 10-15 15:42:55 [model.py:646] Resolved architecture: Qwen3MoeForCausalLM
[36m(_get_vllm_engine_config pid=351782)[0m INFO 10-15 15:42:56 [model.py:1734] Using max model len 40960
[36m(_get_vllm_engine_config pid=351782)[0m INFO 10-15 15:42:56 [arg_utils.py:1348] Using ray runtime env (env vars redacted): {'cgroupv2': {}, 'ray_debugger': {'working_dir': '/home/ray/default/workspace'}, 'working_dir': 'gcs://_ray_pkg_bf7cd164f6f19ed805ef39e17f89ed25bad202fb.zip', 'env_vars': {'AWS_ACCESS_KEY_ID': '***', 'AWS_SECRET_ACCESS_KEY': '***', 'AWS_SESSION_TOKEN': '***', 'VLLM_USE_V1': '***'}, 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}
[36m(_get_vllm_engine_config pid=351782)[0m INFO 10-15 15:42:56 [scheduler.py:225] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m WARNING 10-15 15:42:57 [__init__.py:2881] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:43:01 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:03 [core.py:734] Waiting for init message from front-end.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:03 [core.py:97] Initializing a V1 LLM engine (v0.1.dev10446+g2dcd12d35) with config: model='Qwen/Qwen3-235B-A22B', speculative_config=None, tokenizer='Qwen/Qwen3-235B-A22B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llama, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': 3, 'debug_dump_path': None, 'cache_dir': '/home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention', 'vllm::sparse_attn_indexer'], 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 512, 'local_cache_dir': None}
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:03 [ray_utils.py:351] Using the existing placement group
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:03 [ray_distributed_executor.py:180] use_ray_spmd_worker: True
[36m(pid=352653)[0m INFO 10-15 15:43:09 [__init__.py:224] Automatically detected platform cuda.
[36m(pid=352655)[0m INFO 10-15 15:43:09 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:14 [ray_env.py:66] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:14 [ray_env.py:69] Copying the following environment variables to workers: ['VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_V1', 'VLLM_USE_RAY_COMPILED_DAG', 'LD_LIBRARY_PATH']
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:43:14 [ray_env.py:74] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=352656)[0m WARNING 10-15 15:43:26 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(pid=352653)[0m INFO 10-15 15:43:09 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=352658)[0m [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(RayWorkerWrapper pid=352658)[0m [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:43:29 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:34 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(RayWorkerWrapper pid=352652)[0m WARNING 10-15 15:43:26 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352658)[0m [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 18x across cluster][0m
[36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:43:34 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_f17536f5'), local_subscribe_addr='ipc:///tmp/2262b082-2976-4a48-a0c5-fd357dcdb6ce', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:43:34 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:37 [parallel_state.py:1325] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[36m(RayWorkerWrapper pid=352654)[0m INFO 10-15 15:43:37 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:37 [gpu_model_runner.py:2845] Starting to load model Qwen/Qwen3-235B-A22B...
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:38 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:38 [weight_utils.py:419] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=352655)[0m INFO 10-15 15:55:00 [weight_utils.py:440] Time spent downloading weights for Qwen/Qwen3-235B-A22B: 682.151174 seconds
[36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:43:34 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352656)[0m [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 28x across cluster][0m
[36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:43:37 [parallel_state.py:1325] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:43:37 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:43:37 [gpu_model_runner.py:2845] Starting to load model Qwen/Qwen3-235B-A22B...[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:43:38 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:43:38 [weight_utils.py:419] Using model weights format ['*.safetensors'][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:55:33 [default_loader.py:314] Loading weights took 31.97 seconds
[36m(RayWorkerWrapper pid=352654)[0m INFO 10-15 15:55:33 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 715.113196 seconds
[36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:55:52 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_5_0/backbone for vLLM's torch.compile
[36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:55:52 [backends.py:622] Dynamo bytecode transform time: 16.74 s
[36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:55:34 [default_loader.py:314] Loading weights took 32.83 seconds[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:55:35 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 716.166863 seconds[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:55:58 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.530 s
[36m(RayWorkerWrapper pid=352652)[0m INFO 10-15 15:55:52 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_2_0/backbone for vLLM's torch.compile[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352652)[0m INFO 10-15 15:55:52 [backends.py:622] Dynamo bytecode transform time: 17.06 s[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:56:01 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:56:12 [monitor.py:33] torch.compile takes 22.71 s in total
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:55:59 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.702 s[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:56:01 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:56:15 [gpu_worker.py:315] Available KV cache memory: 13.21 GiB
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m WARNING 10-15 15:43:26 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(pid=352658)[0m INFO 10-15 15:43:10 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:43:29 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:34 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352652)[0m WARNING 10-15 15:43:26 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 18x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:43:34 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_f17536f5'), local_subscribe_addr='ipc:///tmp/2262b082-2976-4a48-a0c5-fd357dcdb6ce', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:43:34 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:37 [parallel_state.py:1325] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352654)[0m INFO 10-15 15:43:37 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:37 [gpu_model_runner.py:2845] Starting to load model Qwen/Qwen3-235B-A22B...
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:38 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:43:38 [weight_utils.py:419] Using model weights format ['*.safetensors']
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352655)[0m INFO 10-15 15:55:00 [weight_utils.py:440] Time spent downloading weights for Qwen/Qwen3-235B-A22B: 682.151174 seconds
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:43:34 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 28x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:43:37 [parallel_state.py:1325] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:43:37 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:43:37 [gpu_model_runner.py:2845] Starting to load model Qwen/Qwen3-235B-A22B...[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:43:38 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:43:38 [weight_utils.py:419] Using model weights format ['*.safetensors'][32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352657)[0m INFO 10-15 15:55:33 [default_loader.py:314] Loading weights took 31.97 seconds
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352654)[0m INFO 10-15 15:55:33 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 715.113196 seconds
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:55:52 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_5_0/backbone for vLLM's torch.compile
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:55:52 [backends.py:622] Dynamo bytecode transform time: 16.74 s
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:55:34 [default_loader.py:314] Loading weights took 32.83 seconds[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:55:35 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 716.166863 seconds[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352652)[0m INFO 10-15 15:55:52 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_2_0/backbone for vLLM's torch.compile[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352652)[0m INFO 10-15 15:55:52 [backends.py:622] Dynamo bytecode transform time: 17.06 s[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:55:59 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.702 s[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:56:01 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:56:16 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:57:00 [custom_all_reduce.py:216] Registering 2268 cuda graph addresses
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:56:12 [monitor.py:33] torch.compile takes 22.71 s in total[32m [repeated 8x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:55:58 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.530 s
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:56:01 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352651)[0m INFO 10-15 15:56:15 [gpu_worker.py:315] Available KV cache memory: 13.21 GiB[32m [repeated 8x across cluster][0m
[36m(RayWorkerWrapper pid=352658)[0m INFO 10-15 15:57:00 [gpu_model_runner.py:3816] Graph capturing finished in 44 secs, took 2.99 GiB
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352656)[0m INFO 10-15 15:56:12 [monitor.py:33] torch.compile takes 22.31 s in total[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m [36m(RayWorkerWrapper pid=352653)[0m INFO 10-15 15:56:16 [gpu_worker.py:315] Available KV cache memory: 13.21 GiB[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:57:01 [core.py:243] init engine (profile, create kv cache, warmup model) took 86.76 seconds
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:02 [loggers.py:191] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 18426
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:57:02 [gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:02 [api_server.py:1629] Supported tasks: ['generate']
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m WARNING 10-15 15:57:02 [model.py:1591] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:02 [serving_responses.py:148] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:02 [serving_chat.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:02 [serving_completion.py:67] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:03 [chat_utils.py:545] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m INFO 10-15 15:57:03 [async_llm.py:344] Added request chatcmpl-02462e16-6571-4210-8146-c331d6ceb4a1.
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:57:03 [ray_distributed_executor.py:570] RAY_CGRAPH_get_timeout is set to 300
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:57:03 [ray_distributed_executor.py:574] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:57:03 [ray_distributed_executor.py:578] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False
[36m(ServeReplica:default:LLMServer:llama pid=351565)[0m [1;36m(EngineCore_DP0 pid=352530)[0;0m INFO 10-15 15:57:03 [ray_distributed_executor.py:654] Using RayPPCommunicator (which wraps vLLM _PP GroupCoordinator) for Ray Compiled Graph communication.
