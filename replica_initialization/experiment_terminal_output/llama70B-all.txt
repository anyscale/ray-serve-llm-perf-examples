INFO 10-15 13:56:48 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:OpenAiIngress pid=218161)[0m INFO 10-15 13:59:08 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 13:59:14 [__init__.py:224] Automatically detected platform cuda.
[36m(download_model_files pid=218617)[0m INFO 10-15 13:59:21 [__init__.py:224] Automatically detected platform cuda.
[36m(_get_vllm_engine_config pid=218617)[0m INFO 10-15 13:59:28 [model.py:646] Resolved architecture: LlamaForCausalLM
[36m(_get_vllm_engine_config pid=218617)[0m INFO 10-15 13:59:29 [model.py:1734] Using max model len 8192
[36m(_get_vllm_engine_config pid=218617)[0m INFO 10-15 13:59:29 [arg_utils.py:1348] Using ray runtime env (env vars redacted): {'cgroupv2': {}, 'ray_debugger': {'working_dir': '/home/ray/default/workspace'}, 'working_dir': 'gcs://_ray_pkg_44b9f4a4302d4745e42e739de8e15f5251f9cc51.zip', 'env_vars': {'AWS_ACCESS_KEY_ID': '***', 'AWS_SECRET_ACCESS_KEY': '***', 'AWS_SESSION_TOKEN': '***', 'VLLM_USE_V1': '***'}, 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}
[36m(_get_vllm_engine_config pid=218617)[0m INFO 10-15 13:59:29 [scheduler.py:225] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m WARNING 10-15 13:59:30 [__init__.py:2881] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 13:59:33 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:35 [core.py:734] Waiting for init message from front-end.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:35 [core.py:97] Initializing a V1 LLM engine (v0.1.dev10446+g2dcd12d35) with config: model='/home/ray/.cache/vllm/assets/model_streamer/c3e8d69a', speculative_config=None, tokenizer='/home/ray/.cache/vllm/assets/model_streamer/c3e8d69a', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=runai_streamer_sharded, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llama, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': 3, 'debug_dump_path': None, 'cache_dir': '/home/ray/.cache/vllm/torch_compile_cache/llama-3-70b-cache', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention', 'vllm::sparse_attn_indexer'], 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 512, 'local_cache_dir': None}
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:35 [ray_utils.py:351] Using the existing placement group
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:35 [ray_distributed_executor.py:180] use_ray_spmd_worker: True
[36m(pid=219112)[0m INFO 10-15 13:59:40 [__init__.py:224] Automatically detected platform cuda.
[36m(pid=219113)[0m INFO 10-15 13:59:40 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:43 [ray_env.py:66] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:43 [ray_env.py:69] Copying the following environment variables to workers: ['VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 13:59:43 [ray_env.py:74] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=219112)[0m WARNING 10-15 13:59:50 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(pid=219112)[0m INFO 10-15 13:59:40 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=219112)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(RayWorkerWrapper pid=219112)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:51 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:54 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:54 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_26e261ae'), local_subscribe_addr='ipc:///tmp/1636ee4c-dfd7-4fde-ac17-619e04708267', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:54 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:55 [parallel_state.py:1325] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[36m(RayWorkerWrapper pid=219113)[0m WARNING 10-15 13:59:50 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:56 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:56 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/c3e8d69a...
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:56 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=219113)[0m [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[32m [repeated 22x across cluster][0m
[36m(RayWorkerWrapper pid=219112)[0m [RunAI Streamer] CPU Buffer size: 56 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-6.safetensors']
[36m(RayWorkerWrapper pid=219112)[0m [RunAI Streamer] CPU Buffer size: 55.1 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-6.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-0.safetensors']
[36m(RayWorkerWrapper pid=219112)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-6.safetensors']
[36m(RayWorkerWrapper pid=219112)[0m Read throughput is 1.34 GB per second 
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:54 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:55 [parallel_state.py:1325] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:56 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:56 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/c3e8d69a...[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:56 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219115)[0m [RunAI Streamer] CPU Buffer size: 56 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-6.safetensors'][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m [RunAI Streamer] CPU Buffer size: 55.1 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-6.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-0.safetensors'][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-6.safetensors'][32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m WARNING 10-15 13:59:50 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(pid=219111)[0m INFO 10-15 13:59:41 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:51 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:54 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_26e261ae'), local_subscribe_addr='ipc:///tmp/1636ee4c-dfd7-4fde-ac17-619e04708267', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:54 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m WARNING 10-15 13:59:50 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[32m [repeated 22x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:54 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:55 [parallel_state.py:1325] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 13:59:56 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:56 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/c3e8d69a...[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 13:59:56 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219115)[0m [RunAI Streamer] CPU Buffer size: 56 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-0-part-6.safetensors'][32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m [RunAI Streamer] CPU Buffer size: 55.1 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-6.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-0.safetensors'][32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-1-part-6.safetensors'][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m [RunAI Streamer] Overall time to stream 32.9 GiB of all files: 25.6s, 1.3 GiB/s
[36m(RayWorkerWrapper pid=219113)[0m Read throughput is 1.60 GB per second [32m [repeated 5x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:54 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:55 [parallel_state.py:1325] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:56 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:56 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/c3e8d69a...
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 13:59:56 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m [RunAI Streamer] CPU Buffer size: 56 Bytes for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-6.safetensors']
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m [RunAI Streamer] CPU Buffer size: 55.1 KiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-6.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-0.safetensors']
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-0.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Meta-Llama-3-70B-sharded/model-rank-2-part-6.safetensors']
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:23 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 26.455494 seconds
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:36 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/llama-3-70b-cache/rank_2_0/backbone for vLLM's torch.compile
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:36 [backends.py:622] Dynamo bytecode transform time: 11.71 s
[36m(RayWorkerWrapper pid=219111)[0m [RunAI Streamer] Overall time to stream 32.9 GiB of all files: 26.72s, 1.2 GiB/s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219111)[0m Read throughput is 2.23 GB per second [32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219111)[0m INFO 10-15 14:00:24 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 27.513809 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:41 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.938 s
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:37 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/llama-3-70b-cache/rank_1_0/backbone for vLLM's torch.compile[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:37 [backends.py:622] Dynamo bytecode transform time: 12.24 s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:43 [monitor.py:33] torch.compile takes 15.65 s in total
[36m(RayWorkerWrapper pid=219111)[0m INFO 10-15 14:00:45 [gpu_worker.py:315] Available KV cache memory: 35.76 GiB
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m 
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m [RunAI Streamer] Overall time to stream 32.9 GiB of all files: 25.6s, 1.3 GiB/s
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m Read throughput is 1.60 GB per second [32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:23 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 26.455494 seconds
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219111)[0m [RunAI Streamer] Overall time to stream 32.9 GiB of all files: 26.72s, 1.2 GiB/s[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219111)[0m Read throughput is 2.23 GB per second [32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219111)[0m INFO 10-15 14:00:24 [gpu_model_runner.py:2906] Model loading took 32.8601 GiB and 27.513809 seconds[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:37 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/llama-3-70b-cache/rank_1_0/backbone for vLLM's torch.compile[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:37 [backends.py:622] Dynamo bytecode transform time: 12.24 s[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1199] GPU KV cache size: 468,752 tokens
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:00:46 [kv_cache_utils.py:1204] Maximum concurrency for 8,192 tokens per request: 57.22x
[36m(RayWorkerWrapper pid=219115)[0m INFO 10-15 14:01:01 [custom_all_reduce.py:216] Registering 21252 cuda graph addresses
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:41 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.938 s[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:36 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/llama-3-70b-cache/rank_2_0/backbone for vLLM's torch.compile
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:36 [backends.py:622] Dynamo bytecode transform time: 11.71 s
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:00:43 [monitor.py:33] torch.compile takes 15.65 s in total[32m [repeated 4x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219111)[0m INFO 10-15 14:00:45 [gpu_worker.py:315] Available KV cache memory: 35.76 GiB[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=219112)[0m INFO 10-15 14:01:02 [gpu_model_runner.py:3816] Graph capturing finished in 16 secs, took 1.05 GiB
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:41 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.860 s[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:43 [monitor.py:33] torch.compile takes 16.10 s in total[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m [36m(RayWorkerWrapper pid=219113)[0m INFO 10-15 14:00:45 [gpu_worker.py:315] Available KV cache memory: 35.76 GiB[32m [repeated 3x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:01:02 [core.py:243] init engine (profile, create kv cache, warmup model) took 38.11 seconds
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 14:01:03 [loggers.py:191] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 29297
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m [1;36m(EngineCore_DP0 pid=218993)[0;0m INFO 10-15 14:01:03 [gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 14:01:03 [api_server.py:1629] Supported tasks: ['generate']
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m WARNING 10-15 14:01:03 [model.py:1591] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 14:01:03 [serving_responses.py:148] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 14:01:03 [serving_chat.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 14:01:03 [serving_completion.py:67] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m INFO 10-15 14:01:04 [chat_utils.py:545] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256] Error in preprocessing prompt inputs
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256] Traceback (most recent call last):
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]   File "/home/ray/default/repo/vllm/vllm/entrypoints/openai/serving_chat.py", line 234, in create_chat_completion
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]     ) = await self._preprocess_chat(
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]   File "/home/ray/default/repo/vllm/vllm/entrypoints/openai/serving_engine.py", line 1085, in _preprocess_chat
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]     request_prompt = apply_hf_chat_template(
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]                      ^^^^^^^^^^^^^^^^^^^^^^^
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]   File "/home/ray/default/repo/vllm/vllm/entrypoints/chat_utils.py", line 1535, in apply_hf_chat_template
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256]     raise ValueError(
[36m(ServeReplica:default:LLMServer:llama pid=218379)[0m ERROR 10-15 14:01:04 [serving_chat.py:256] ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.
