INFO 10-15 16:02:16 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:OpenAiIngress pid=369489)[0m INFO 10-15 16:02:43 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:02:48 [__init__.py:224] Automatically detected platform cuda.
[36m(download_model_files pid=369880)[0m INFO 10-15 16:02:56 [__init__.py:224] Automatically detected platform cuda.
[36m(_get_vllm_engine_config pid=369880)[0m INFO 10-15 16:03:38 [model.py:646] Resolved architecture: Qwen3MoeForCausalLM
[36m(_get_vllm_engine_config pid=369880)[0m INFO 10-15 16:03:38 [model.py:1734] Using max model len 40960
[36m(_get_vllm_engine_config pid=369880)[0m INFO 10-15 16:03:38 [arg_utils.py:1348] Using ray runtime env (env vars redacted): {'cgroupv2': {}, 'ray_debugger': {'working_dir': '/home/ray/default/workspace'}, 'working_dir': 'gcs://_ray_pkg_88b8d516982b9d91f5e7650b6aa94fc5f6112020.zip', 'env_vars': {'AWS_ACCESS_KEY_ID': '***', 'AWS_SECRET_ACCESS_KEY': '***', 'AWS_SESSION_TOKEN': '***', 'VLLM_USE_V1': '***'}, 'worker_process_setup_hook': 'ray.llm._internal.serve._worker_process_setup_hook'}
[36m(_get_vllm_engine_config pid=369880)[0m INFO 10-15 16:03:39 [scheduler.py:225] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m WARNING 10-15 16:03:39 [__init__.py:2881] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:03:42 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:45 [core.py:734] Waiting for init message from front-end.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:45 [core.py:97] Initializing a V1 LLM engine (v0.1.dev10446+g2dcd12d35) with config: model='/home/ray/.cache/vllm/assets/model_streamer/a65e7a19', speculative_config=None, tokenizer='/home/ray/.cache/vllm/assets/model_streamer/a65e7a19', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=runai_streamer_sharded, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llama, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': 3, 'debug_dump_path': None, 'cache_dir': '/home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention', 'vllm::sparse_attn_indexer'], 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 512, 'local_cache_dir': None}
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:45 [ray_utils.py:351] Using the existing placement group
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:45 [ray_distributed_executor.py:180] use_ray_spmd_worker: True
[36m(pid=370682)[0m INFO 10-15 16:03:51 [__init__.py:224] Automatically detected platform cuda.
[36m(pid=370669)[0m INFO 10-15 16:03:51 [__init__.py:224] Automatically detected platform cuda.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:55 [ray_env.py:66] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:55 [ray_env.py:69] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_RAY_SPMD_WORKER']
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:03:55 [ray_env.py:74] If certain env vars should NOT be copied, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=370669)[0m WARNING 10-15 16:04:07 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(pid=370682)[0m INFO 10-15 16:03:51 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370679)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(RayWorkerWrapper pid=370679)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:10 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:04:16 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(RayWorkerWrapper pid=370681)[0m WARNING 10-15 16:04:08 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370679)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 18x across cluster][0m
[36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:16 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_93fcf4aa'), local_subscribe_addr='ipc:///tmp/d39d3a7a-eca0-4b0e-b912-87d440b8f9aa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:16 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:04:18 [parallel_state.py:1325] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:04:19 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:04:19 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/a65e7a19...
[36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:19 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=370678)[0m [RunAI Streamer] CPU Buffer size: 96 Bytes for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-9.safetensors']
[36m(RayWorkerWrapper pid=370679)[0m [RunAI Streamer] CPU Buffer size: 95.6 KiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-9.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-3.safetensors']
[36m(RayWorkerWrapper pid=370682)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-9.safetensors']
[36m(RayWorkerWrapper pid=370670)[0m Read throughput is 800.27 MB per second 
[36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:04:16 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370681)[0m [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 28x across cluster][0m
[36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:04:18 [parallel_state.py:1325] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370669)[0m INFO 10-15 16:04:19 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:04:19 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/a65e7a19...[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370669)[0m INFO 10-15 16:04:20 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370680)[0m [RunAI Streamer] CPU Buffer size: 96 Bytes for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-9.safetensors'][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370669)[0m [RunAI Streamer] CPU Buffer size: 95.6 KiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-9.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-11.safetensors'][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370669)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-9.safetensors'][32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370669)[0m WARNING 10-15 16:04:07 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(pid=370681)[0m INFO 10-15 16:03:52 [__init__.py:224] Automatically detected platform cuda.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:10 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m WARNING 10-15 16:04:08 [worker_base.py:309] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 18x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:16 [shm_broadcast.py:302] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_93fcf4aa'), local_subscribe_addr='ipc:///tmp/d39d3a7a-eca0-4b0e-b912-87d440b8f9aa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:16 [pynccl.py:111] vLLM is using nccl==2.27.3
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:04:16 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 28x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:04:18 [parallel_state.py:1325] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370669)[0m INFO 10-15 16:04:19 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:04:19 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/a65e7a19...[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370669)[0m INFO 10-15 16:04:20 [cuda.py:405] Using Flash Attention backend on V1 engine.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370680)[0m [RunAI Streamer] CPU Buffer size: 96 Bytes for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-2-part-9.safetensors'][32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370678)[0m Read throughput is 630.54 MB per second [32m [repeated 8x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:04:16 [custom_all_reduce.py:37] Skipping P2P check and trusting the driver's P2P report.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 2x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:04:18 [parallel_state.py:1325] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:04:19 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:04:19 [gpu_model_runner.py:2845] Starting to load model /home/ray/.cache/vllm/assets/model_streamer/a65e7a19...
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:04:19 [cuda.py:405] Using Flash Attention backend on V1 engine.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m [RunAI Streamer] CPU Buffer size: 96 Bytes for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-0-part-9.safetensors']
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m [RunAI Streamer] CPU Buffer size: 95.6 KiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-9.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-3-part-3.safetensors']
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-6-part-9.safetensors']
[36m(RayWorkerWrapper pid=370680)[0m Read throughput is 811.24 MB per second 
[36m(RayWorkerWrapper pid=370672)[0m Read throughput is 815.41 MB per second 
[36m(RayWorkerWrapper pid=370670)[0m Read throughput is 486.46 MB per second [32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=370682)[0m Read throughput is 814.11 MB per second 
[36m(RayWorkerWrapper pid=370682)[0m [RunAI Streamer] Overall time to stream 54.9 GiB of all files: 74.87s, 751.0 MiB/s
[36m(RayWorkerWrapper pid=370672)[0m Read throughput is 803.70 MB per second 
[36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:05:36 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 75.844352 seconds
[36m(RayWorkerWrapper pid=370670)[0m Read throughput is 1.78 GB per second 
[36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:05:55 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_6_0/backbone for vLLM's torch.compile
[36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:05:55 [backends.py:622] Dynamo bytecode transform time: 16.93 s
[36m(RayWorkerWrapper pid=370670)[0m [RunAI Streamer] Overall time to stream 54.9 GiB of all files: 76.95s, 730.7 MiB/s[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370678)[0m Read throughput is 899.31 MB per second [32m [repeated 5x across cluster][0m
[36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:05:38 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 77.913745 seconds[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370672)[0m INFO 10-15 16:06:02 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.493 s
[36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:05:57 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_5_0/backbone for vLLM's torch.compile[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:05:57 [backends.py:622] Dynamo bytecode transform time: 18.06 s[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:06:06 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.
[36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:06:16 [monitor.py:33] torch.compile takes 22.65 s in total
[36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:06:04 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.795 s[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:06:06 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:06:20 [gpu_worker.py:315] Available KV cache memory: 13.21 GiB
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m 
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370669)[0m [RunAI Streamer] CPU Buffer size: 95.6 KiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-9.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-11.safetensors'][32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370669)[0m [RunAI Streamer] CPU Buffer size: 18.6 GiB for files: ['s3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-0.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-1.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-10.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-11.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-2.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-3.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-4.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-5.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-6.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-7.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-8.safetensors', 's3://ahao-runai-east1/Qwen3-235B-A22B-sharded/model-rank-4-part-9.safetensors'][32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m Read throughput is 630.54 MB per second [32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370680)[0m Read throughput is 811.24 MB per second 
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370672)[0m Read throughput is 815.41 MB per second 
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m Read throughput is 486.46 MB per second [32m [repeated 6x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m Read throughput is 814.11 MB per second 
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m [RunAI Streamer] Overall time to stream 54.9 GiB of all files: 74.87s, 751.0 MiB/s
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370672)[0m Read throughput is 803.70 MB per second 
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:05:36 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 75.844352 seconds
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m Read throughput is 1.78 GB per second 
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:05:55 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_6_0/backbone for vLLM's torch.compile
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370682)[0m INFO 10-15 16:05:55 [backends.py:622] Dynamo bytecode transform time: 16.93 s
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m [RunAI Streamer] Overall time to stream 54.9 GiB of all files: 76.95s, 730.7 MiB/s[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m Read throughput is 899.31 MB per second [32m [repeated 5x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:05:38 [gpu_model_runner.py:2906] Model loading took 54.9205 GiB and 77.913745 seconds[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:05:57 [backends.py:608] Using cache directory: /home/ray/.cache/vllm/torch_compile_cache/Qwen3-235B-cache/rank_5_0/backbone for vLLM's torch.compile[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:05:57 [backends.py:622] Dynamo bytecode transform time: 18.06 s[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:06:04 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.795 s[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:06:06 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1199] GPU KV cache size: 294,816 tokens
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:06:21 [kv_cache_utils.py:1204] Maximum concurrency for 40,960 tokens per request: 7.20x
[36m(RayWorkerWrapper pid=370670)[0m INFO 10-15 16:07:05 [custom_all_reduce.py:216] Registering 2268 cuda graph addresses
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:06:16 [monitor.py:33] torch.compile takes 22.65 s in total[32m [repeated 8x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370672)[0m INFO 10-15 16:06:02 [backends.py:206] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.493 s
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370679)[0m INFO 10-15 16:06:06 [fused_moe.py:863] Using configuration from /home/ray/default/repo/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:06:20 [gpu_worker.py:315] Available KV cache memory: 13.21 GiB[32m [repeated 8x across cluster][0m
[36m(RayWorkerWrapper pid=370678)[0m INFO 10-15 16:07:05 [gpu_model_runner.py:3816] Graph capturing finished in 44 secs, took 2.99 GiB
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370681)[0m INFO 10-15 16:06:16 [monitor.py:33] torch.compile takes 23.39 s in total[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m [36m(RayWorkerWrapper pid=370680)[0m INFO 10-15 16:06:20 [gpu_worker.py:315] Available KV cache memory: 13.21 GiB[32m [repeated 7x across cluster][0m
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:07:06 [core.py:243] init engine (profile, create kv cache, warmup model) took 88.20 seconds
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:07 [loggers.py:191] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 18426
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:07:07 [gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:07 [api_server.py:1629] Supported tasks: ['generate']
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m WARNING 10-15 16:07:07 [model.py:1591] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:07 [serving_responses.py:148] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:07 [serving_chat.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:07 [serving_completion.py:67] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:08 [chat_utils.py:545] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m INFO 10-15 16:07:08 [async_llm.py:344] Added request chatcmpl-a919da77-bf8e-4568-8afd-7cb1fe10b5f4.
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:07:08 [ray_distributed_executor.py:570] RAY_CGRAPH_get_timeout is set to 300
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:07:08 [ray_distributed_executor.py:574] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:07:08 [ray_distributed_executor.py:578] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False
[36m(ServeReplica:default:LLMServer:llama pid=369678)[0m [1;36m(EngineCore_DP0 pid=370549)[0;0m INFO 10-15 16:07:08 [ray_distributed_executor.py:654] Using RayPPCommunicator (which wraps vLLM _PP GroupCoordinator) for Ray Compiled Graph communication.
